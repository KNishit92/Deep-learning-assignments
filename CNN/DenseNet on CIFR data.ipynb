{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wVIx_KIigxPV"
      },
      "outputs": [],
      "source": [
        "# import keras\n",
        "# from keras.datasets import cifar10\n",
        "# from keras.models import Model, Sequential\n",
        "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "# from keras.layers import Concatenate\n",
        "# from keras.optimizers import Adam\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UNHw6luQg3gc"
      },
      "outputs": [],
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dsO_yGxcg5D8"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 300\n",
        "l = 6\n",
        "num_filter = 30\n",
        "compression = 1.0\n",
        "# dropout_rate = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8LGpxpCkAOq",
        "outputId": "cc43b15a-eea7-48a3-cb69-0a1595325610"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "img_width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB7o3zu1g6eT",
        "outputId": "aa965218-f080-49ea-bf19-63592af3706a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR10 Data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sxIPgaVXrX6U"
      },
      "outputs": [],
      "source": [
        "# Normalize the images\n",
        "x_train, x_test = X_train / 255.0, X_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2FcyP-hziezB"
      },
      "outputs": [],
      "source": [
        "# Perform Image Augmentation\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Reference - https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow(\n",
        "        x_train, y = y_train,\n",
        "        batch_size=32) \n",
        "\n",
        "# # this is a similar generator, for validation data\n",
        "# validation_generator = test_datagen.flow(\n",
        "#         X_test, y = y_test,\n",
        "#         batch_size=50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lAk_Mw_5-rn",
        "outputId": "3a34f350-89eb-46c6-e8a8-bce694ee2bcc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVkpgHsc5-rp",
        "outputId": "1c9695ed-342e-459f-8419-e5c091ad0d62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ee-sge5Kg7vr"
      },
      "outputs": [],
      "source": [
        "# Dense Block\n",
        "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Blosck\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    # output = layers.Dense(num_classes, activation='softmax')(flat)\n",
        "    conv = layers.Conv2D(num_classes, (2,2), strides=(1,1), activation='softmax')(AvgPooling)\n",
        "    output = layers.Flatten()(conv)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "anPCpQWhhGb7"
      },
      "outputs": [],
      "source": [
        "# num_filter = 12\n",
        "# dropout_rate = 0.2\n",
        "num_filter = 34\n",
        "dropout_rate = 0.0\n",
        "\n",
        "l = 6\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = transition(First_Block, 64, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "# num_filter = 12\n",
        "# dropout_rate = 0.0\n",
        "\n",
        "# l = 6\n",
        "# input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "# First_Conv2D = layers.Conv2D(32, (5,5), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# First_Block = denseblock(First_Conv2D, 10, dropout_rate)\n",
        "# First_Transition = transition(First_Block, 64, dropout_rate)\n",
        "\n",
        "# Second_Block = denseblock(First_Transition, 10, dropout_rate)\n",
        "# Second_Transition = transition(Second_Block, 32, dropout_rate)\n",
        "\n",
        "# Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "# Third_Transition = transition(Third_Block, 32, dropout_rate)\n",
        "\n",
        "# Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "# output = output_layer(Last_Block)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "c5Wksy8z5-rw",
        "outputId": "f7e8ca62-5e72-4d5e-e741-edfe88cfde83"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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\n",
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/-W6y8xnd--U\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f844c8b1898>"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#https://arxiv.org/pdf/1608.06993.pdf\n",
        "from IPython.display import IFrame, YouTubeVideo\n",
        "YouTubeVideo(id='-W6y8xnd--U', width=600)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('my_model_weights.h5')"
      ],
      "metadata": {
        "id": "BSAJdqFyplwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kFh7pdxhNtT",
        "outputId": "b93f04c4-2f3d-474e-853d-255150aa0753",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 34)   918         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 34)  136         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 34)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 34)   10404       ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32, 32, 68)   0           ['conv2d[0][0]',                 \n",
            "                                                                  'conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 68)  272         ['concatenate[0][0]']            \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 68)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 34)   20808       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 32, 32, 102)  0           ['concatenate[0][0]',            \n",
            "                                                                  'conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 102)  408        ['concatenate_1[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 102)  0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 32, 34)   31212       ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 32, 32, 136)  0           ['concatenate_1[0][0]',          \n",
            "                                                                  'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 136)  544        ['concatenate_2[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 32, 136)  0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 34)   41616       ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 32, 32, 170)  0           ['concatenate_2[0][0]',          \n",
            "                                                                  'conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 170)  680        ['concatenate_3[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 170)  0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 34)   52020       ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 32, 32, 204)  0           ['concatenate_3[0][0]',          \n",
            "                                                                  'conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 204)  816        ['concatenate_4[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 204)  0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 34)   62424       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 32, 32, 238)  0           ['concatenate_4[0][0]',          \n",
            "                                                                  'conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 238)  952        ['concatenate_5[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 238)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 64)   15232       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 16, 16, 64)  0           ['conv2d_7[0][0]']               \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 16, 16, 64)  256         ['average_pooling2d[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 16, 16, 34)   19584       ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 16, 16, 98)   0           ['average_pooling2d[0][0]',      \n",
            "                                                                  'conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 16, 16, 98)  392         ['concatenate_6[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 16, 16, 98)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 16, 16, 34)   29988       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 16, 16, 132)  0           ['concatenate_6[0][0]',          \n",
            "                                                                  'conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 16, 132)  528        ['concatenate_7[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 16, 16, 132)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 16, 16, 34)   40392       ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 16, 16, 166)  0           ['concatenate_7[0][0]',          \n",
            "                                                                  'conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 16, 16, 166)  664        ['concatenate_8[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 16, 16, 166)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 16, 16, 34)   50796       ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 16, 16, 200)  0           ['concatenate_8[0][0]',          \n",
            "                                                                  'conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 200)  800        ['concatenate_9[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 16, 16, 200)  0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 16, 16, 34)   61200       ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 16, 16, 234)  0           ['concatenate_9[0][0]',          \n",
            "                                                                  'conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 234)  936        ['concatenate_10[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 16, 16, 234)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 16, 16, 34)   71604       ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 16, 16, 268)  0           ['concatenate_10[0][0]',         \n",
            "                                                                  'conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 268)  1072       ['concatenate_11[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 268)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 34)   9112        ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 8, 8, 34)    0           ['conv2d_14[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 8, 8, 34)    136         ['average_pooling2d_1[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 8, 8, 34)     0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 8, 8, 34)     10404       ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 8, 8, 68)     0           ['average_pooling2d_1[0][0]',    \n",
            "                                                                  'conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 8, 8, 68)    272         ['concatenate_12[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 8, 8, 68)     0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 8, 8, 34)     20808       ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 8, 8, 102)    0           ['concatenate_12[0][0]',         \n",
            "                                                                  'conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 8, 8, 102)   408         ['concatenate_13[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 8, 8, 102)    0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 8, 8, 34)     31212       ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 8, 8, 136)    0           ['concatenate_13[0][0]',         \n",
            "                                                                  'conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 8, 8, 136)   544         ['concatenate_14[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 8, 8, 136)    0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 8, 8, 34)     41616       ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 8, 8, 170)    0           ['concatenate_14[0][0]',         \n",
            "                                                                  'conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 8, 8, 170)   680         ['concatenate_15[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 8, 8, 170)    0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 8, 8, 34)     52020       ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 8, 8, 204)    0           ['concatenate_15[0][0]',         \n",
            "                                                                  'conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 8, 8, 204)   816         ['concatenate_16[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 8, 8, 204)    0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 8, 8, 34)     62424       ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 8, 8, 238)    0           ['concatenate_16[0][0]',         \n",
            "                                                                  'conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 8, 8, 238)   952         ['concatenate_17[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 8, 8, 238)    0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 8, 8, 34)     8092        ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, 4, 4, 34)    0           ['conv2d_21[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 4, 4, 34)    136         ['average_pooling2d_2[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 4, 4, 34)     0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 4, 4, 34)     10404       ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 4, 4, 68)     0           ['average_pooling2d_2[0][0]',    \n",
            "                                                                  'conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 4, 4, 68)    272         ['concatenate_18[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 4, 4, 68)     0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 4, 4, 34)     20808       ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 4, 4, 102)    0           ['concatenate_18[0][0]',         \n",
            "                                                                  'conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 4, 4, 102)   408         ['concatenate_19[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 4, 4, 102)    0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 4, 4, 34)     31212       ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 4, 4, 136)    0           ['concatenate_19[0][0]',         \n",
            "                                                                  'conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 4, 4, 136)   544         ['concatenate_20[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 4, 4, 136)    0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 4, 4, 34)     41616       ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 4, 4, 170)    0           ['concatenate_20[0][0]',         \n",
            "                                                                  'conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 4, 4, 170)   680         ['concatenate_21[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 4, 4, 170)    0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 4, 4, 34)     52020       ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 4, 4, 204)    0           ['concatenate_21[0][0]',         \n",
            "                                                                  'conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 4, 4, 204)   816         ['concatenate_22[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 4, 4, 204)    0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 4, 4, 34)     62424       ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 4, 4, 238)    0           ['concatenate_22[0][0]',         \n",
            "                                                                  'conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 4, 4, 238)   952         ['concatenate_23[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 4, 4, 238)    0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, 2, 2, 238)   0           ['activation_27[0][0]']          \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 1, 1, 10)     9530        ['average_pooling2d_3[0][0]']    \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 10)           0           ['conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 987,972\n",
            "Trainable params: 979,936\n",
            "Non-trainable params: 8,036\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Aqzk9AFXb1y",
        "outputId": "7bcd13df-524b-4351-ce58-9513eb4af22a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115\n"
          ]
        }
      ],
      "source": [
        "print(len(model.layers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Ko0FBkIWUIq8"
      },
      "outputs": [],
      "source": [
        "# utility functions\n",
        "# decay of weights\n",
        "def reduceLearningRate(epoch, lr):\n",
        "    if (epoch+1) % 10 == 0 and epoch > 9:\n",
        "        return 0.95* lr\n",
        "    return lr\n",
        "\n",
        "# Custom callbacks\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TerminateOnNaN\n",
        "\n",
        "\n",
        "# decay learning rate by 5% for every 10th epoch\n",
        "lrschedule = LearningRateScheduler(reduceLearningRate, verbose=1)\n",
        "# decay learning rate by 10% If validation accuracy at that epoch is less than previous 3 epochs' accuracy\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.9, patience=3, min_delta=0.0001, mode = 'auto')\n",
        "\n",
        "NanStop = TerminateOnNaN()\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('weights{epoch:08d}.h5', save_weights_only=True, save_freq=5)\n",
        "\n",
        "# https://stackoverflow.com/questions/59563085/how-to-stop-training-when-it-hits-a-specific-validation-accuracy\n",
        "class MyThresholdCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, threshold):\n",
        "        super(MyThresholdCallback, self).__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None): \n",
        "        val_acc = logs[\"val_accuracy\"]\n",
        "        if val_acc >= self.threshold:\n",
        "            self.model.stop_training = True\n",
        "\n",
        "stop_training = MyThresholdCallback(threshold=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "b4XOsW3ahSkL"
      },
      "outputs": [],
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.SGD(learning_rate= 0.01, momentum=0.9, nesterov=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZQ4hDIlpgNu",
        "outputId": "2089c10c-b3c9-44ef-d599-ea2a43f3aacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ],
      "source": [
        "print(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crhGk7kEhXAz",
        "outputId": "1f4fdfbb-9030-48c1-b68b-1094264fa315",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/200\n",
            "   5/1562 [..............................] - ETA: 2:38 - loss: 2.4510 - accuracy: 0.1312WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0339s vs `on_train_batch_end` time: 0.0495s). Check your callbacks.\n",
            "1562/1562 [==============================] - 125s 78ms/step - loss: 1.7434 - accuracy: 0.3685 - val_loss: 2.1275 - val_accuracy: 0.3725 - lr: 0.0100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 1.3828 - accuracy: 0.5014 - val_loss: 1.3520 - val_accuracy: 0.5362 - lr: 0.0100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 1.1870 - accuracy: 0.5767 - val_loss: 1.3202 - val_accuracy: 0.5691 - lr: 0.0100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 1.0476 - accuracy: 0.6290 - val_loss: 0.9373 - val_accuracy: 0.6816 - lr: 0.0100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.9551 - accuracy: 0.6652 - val_loss: 1.1603 - val_accuracy: 0.6421 - lr: 0.0100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.8861 - accuracy: 0.6918 - val_loss: 0.9102 - val_accuracy: 0.6998 - lr: 0.0100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.8305 - accuracy: 0.7098 - val_loss: 0.7859 - val_accuracy: 0.7455 - lr: 0.0100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.7840 - accuracy: 0.7276 - val_loss: 0.8140 - val_accuracy: 0.7261 - lr: 0.0100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.7494 - accuracy: 0.7395 - val_loss: 0.7580 - val_accuracy: 0.7448 - lr: 0.0100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.7070 - accuracy: 0.7540 - val_loss: 0.7493 - val_accuracy: 0.7573 - lr: 0.0100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.6851 - accuracy: 0.7614 - val_loss: 0.7425 - val_accuracy: 0.7618 - lr: 0.0100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/200\n",
            "1562/1562 [==============================] - 123s 79ms/step - loss: 0.6578 - accuracy: 0.7740 - val_loss: 0.7924 - val_accuracy: 0.7423 - lr: 0.0100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 0.6308 - accuracy: 0.7807 - val_loss: 0.6271 - val_accuracy: 0.7877 - lr: 0.0100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.6119 - accuracy: 0.7879 - val_loss: 0.8721 - val_accuracy: 0.7292 - lr: 0.0100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.5952 - accuracy: 0.7946 - val_loss: 0.5611 - val_accuracy: 0.8152 - lr: 0.0100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 16/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.5770 - accuracy: 0.8001 - val_loss: 0.6699 - val_accuracy: 0.7798 - lr: 0.0100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 17/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.5583 - accuracy: 0.8057 - val_loss: 0.5897 - val_accuracy: 0.8018 - lr: 0.0100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 18/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.5454 - accuracy: 0.8100 - val_loss: 0.6071 - val_accuracy: 0.7996 - lr: 0.0100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.008999999612569809.\n",
            "Epoch 19/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.5196 - accuracy: 0.8195 - val_loss: 0.5518 - val_accuracy: 0.8207 - lr: 0.0090\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.008549999631941318.\n",
            "Epoch 20/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.5066 - accuracy: 0.8244 - val_loss: 0.4843 - val_accuracy: 0.8370 - lr: 0.0085\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.008549999445676804.\n",
            "Epoch 21/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.4874 - accuracy: 0.8313 - val_loss: 0.5513 - val_accuracy: 0.8211 - lr: 0.0085\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.008549999445676804.\n",
            "Epoch 22/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.4813 - accuracy: 0.8322 - val_loss: 0.5179 - val_accuracy: 0.8314 - lr: 0.0085\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.008549999445676804.\n",
            "Epoch 23/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.4652 - accuracy: 0.8376 - val_loss: 0.5607 - val_accuracy: 0.8229 - lr: 0.0085\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.007694999687373638.\n",
            "Epoch 24/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 0.4522 - accuracy: 0.8440 - val_loss: 0.5267 - val_accuracy: 0.8294 - lr: 0.0077\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.007694999687373638.\n",
            "Epoch 25/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.4476 - accuracy: 0.8428 - val_loss: 0.5011 - val_accuracy: 0.8367 - lr: 0.0077\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.007694999687373638.\n",
            "Epoch 26/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.4386 - accuracy: 0.8475 - val_loss: 0.5224 - val_accuracy: 0.8328 - lr: 0.0077\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.006925499532371759.\n",
            "Epoch 27/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.4281 - accuracy: 0.8505 - val_loss: 0.4765 - val_accuracy: 0.8466 - lr: 0.0069\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.006925499532371759.\n",
            "Epoch 28/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.4167 - accuracy: 0.8552 - val_loss: 0.5351 - val_accuracy: 0.8304 - lr: 0.0069\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.006925499532371759.\n",
            "Epoch 29/200\n",
            "1562/1562 [==============================] - 123s 79ms/step - loss: 0.4048 - accuracy: 0.8595 - val_loss: 0.5051 - val_accuracy: 0.8393 - lr: 0.0069\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.006579224555753171.\n",
            "Epoch 30/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.3969 - accuracy: 0.8620 - val_loss: 0.4932 - val_accuracy: 0.8435 - lr: 0.0066\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.005921301897615194.\n",
            "Epoch 31/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.3862 - accuracy: 0.8661 - val_loss: 0.4519 - val_accuracy: 0.8525 - lr: 0.0059\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.005921301897615194.\n",
            "Epoch 32/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.3802 - accuracy: 0.8680 - val_loss: 0.4521 - val_accuracy: 0.8550 - lr: 0.0059\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.005921301897615194.\n",
            "Epoch 33/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 0.3758 - accuracy: 0.8687 - val_loss: 0.4619 - val_accuracy: 0.8536 - lr: 0.0059\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.005921301897615194.\n",
            "Epoch 34/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.3757 - accuracy: 0.8684 - val_loss: 0.4152 - val_accuracy: 0.8643 - lr: 0.0059\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.005921301897615194.\n",
            "Epoch 35/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 0.3684 - accuracy: 0.8719 - val_loss: 0.4172 - val_accuracy: 0.8659 - lr: 0.0059\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.005921301897615194.\n",
            "Epoch 36/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.3640 - accuracy: 0.8735 - val_loss: 0.4176 - val_accuracy: 0.8658 - lr: 0.0059\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.005921301897615194.\n",
            "Epoch 37/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.3541 - accuracy: 0.8764 - val_loss: 0.3882 - val_accuracy: 0.8697 - lr: 0.0059\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.005921301897615194.\n",
            "Epoch 38/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.3494 - accuracy: 0.8796 - val_loss: 0.4197 - val_accuracy: 0.8647 - lr: 0.0059\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.005921301897615194.\n",
            "Epoch 39/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.3529 - accuracy: 0.8765 - val_loss: 0.4118 - val_accuracy: 0.8691 - lr: 0.0059\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.005625236802734434.\n",
            "Epoch 40/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.3399 - accuracy: 0.8826 - val_loss: 0.4282 - val_accuracy: 0.8641 - lr: 0.0056\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.005062713287770748.\n",
            "Epoch 41/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.3310 - accuracy: 0.8835 - val_loss: 0.3911 - val_accuracy: 0.8751 - lr: 0.0051\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.005062713287770748.\n",
            "Epoch 42/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.3301 - accuracy: 0.8828 - val_loss: 0.3913 - val_accuracy: 0.8743 - lr: 0.0051\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.005062713287770748.\n",
            "Epoch 43/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.3284 - accuracy: 0.8848 - val_loss: 0.4067 - val_accuracy: 0.8700 - lr: 0.0051\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.005062713287770748.\n",
            "Epoch 44/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.3224 - accuracy: 0.8863 - val_loss: 0.3946 - val_accuracy: 0.8727 - lr: 0.0051\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.004556442145258188.\n",
            "Epoch 45/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 0.3115 - accuracy: 0.8917 - val_loss: 0.4228 - val_accuracy: 0.8653 - lr: 0.0046\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.004556442145258188.\n",
            "Epoch 46/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 0.3074 - accuracy: 0.8936 - val_loss: 0.4378 - val_accuracy: 0.8633 - lr: 0.0046\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.004556442145258188.\n",
            "Epoch 47/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.3048 - accuracy: 0.8932 - val_loss: 0.4202 - val_accuracy: 0.8679 - lr: 0.0046\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.004100797697901726.\n",
            "Epoch 48/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.3006 - accuracy: 0.8956 - val_loss: 0.3894 - val_accuracy: 0.8723 - lr: 0.0041\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.004100797697901726.\n",
            "Epoch 49/200\n",
            "1562/1562 [==============================] - 123s 79ms/step - loss: 0.2967 - accuracy: 0.8953 - val_loss: 0.3937 - val_accuracy: 0.8745 - lr: 0.0041\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.003895757813006639.\n",
            "Epoch 50/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.2886 - accuracy: 0.8997 - val_loss: 0.3823 - val_accuracy: 0.8796 - lr: 0.0039\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 0.003895757719874382.\n",
            "Epoch 51/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.2915 - accuracy: 0.8981 - val_loss: 0.4153 - val_accuracy: 0.8728 - lr: 0.0039\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 0.003895757719874382.\n",
            "Epoch 52/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.2887 - accuracy: 0.8986 - val_loss: 0.3932 - val_accuracy: 0.8799 - lr: 0.0039\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 0.003895757719874382.\n",
            "Epoch 53/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.2800 - accuracy: 0.9014 - val_loss: 0.4491 - val_accuracy: 0.8645 - lr: 0.0039\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 0.003895757719874382.\n",
            "Epoch 54/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.2835 - accuracy: 0.9007 - val_loss: 0.3646 - val_accuracy: 0.8882 - lr: 0.0039\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 0.003895757719874382.\n",
            "Epoch 55/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.2747 - accuracy: 0.9030 - val_loss: 0.3665 - val_accuracy: 0.8854 - lr: 0.0039\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 0.003895757719874382.\n",
            "Epoch 56/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.2793 - accuracy: 0.9031 - val_loss: 0.3306 - val_accuracy: 0.8944 - lr: 0.0039\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 0.003895757719874382.\n",
            "Epoch 57/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.2761 - accuracy: 0.9037 - val_loss: 0.3854 - val_accuracy: 0.8795 - lr: 0.0039\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 0.003895757719874382.\n",
            "Epoch 58/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.2790 - accuracy: 0.9020 - val_loss: 0.4335 - val_accuracy: 0.8691 - lr: 0.0039\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 0.003895757719874382.\n",
            "Epoch 59/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 0.2669 - accuracy: 0.9054 - val_loss: 0.4127 - val_accuracy: 0.8757 - lr: 0.0039\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 0.0033308728947304186.\n",
            "Epoch 60/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.2617 - accuracy: 0.9087 - val_loss: 0.3412 - val_accuracy: 0.8935 - lr: 0.0033\n",
            "\n",
            "Epoch 00061: LearningRateScheduler setting learning rate to 0.0033308728598058224.\n",
            "Epoch 61/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.2606 - accuracy: 0.9078 - val_loss: 0.3413 - val_accuracy: 0.8922 - lr: 0.0033\n",
            "\n",
            "Epoch 00062: LearningRateScheduler setting learning rate to 0.0033308728598058224.\n",
            "Epoch 62/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.2602 - accuracy: 0.9091 - val_loss: 0.3631 - val_accuracy: 0.8863 - lr: 0.0033\n",
            "\n",
            "Epoch 00063: LearningRateScheduler setting learning rate to 0.00299778557382524.\n",
            "Epoch 63/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.2521 - accuracy: 0.9126 - val_loss: 0.4097 - val_accuracy: 0.8765 - lr: 0.0030\n",
            "\n",
            "Epoch 00064: LearningRateScheduler setting learning rate to 0.00299778557382524.\n",
            "Epoch 64/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.2491 - accuracy: 0.9121 - val_loss: 0.3589 - val_accuracy: 0.8886 - lr: 0.0030\n",
            "\n",
            "Epoch 00065: LearningRateScheduler setting learning rate to 0.00299778557382524.\n",
            "Epoch 65/200\n",
            "1562/1562 [==============================] - 121s 78ms/step - loss: 0.2472 - accuracy: 0.9129 - val_loss: 0.3816 - val_accuracy: 0.8854 - lr: 0.0030\n",
            "\n",
            "Epoch 00066: LearningRateScheduler setting learning rate to 0.0026980070397257805.\n",
            "Epoch 66/200\n",
            "1562/1562 [==============================] - 119s 76ms/step - loss: 0.2434 - accuracy: 0.9142 - val_loss: 0.3682 - val_accuracy: 0.8870 - lr: 0.0027\n",
            "\n",
            "Epoch 00067: LearningRateScheduler setting learning rate to 0.0026980070397257805.\n",
            "Epoch 67/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.2408 - accuracy: 0.9159 - val_loss: 0.3914 - val_accuracy: 0.8796 - lr: 0.0027\n",
            "\n",
            "Epoch 00068: LearningRateScheduler setting learning rate to 0.0026980070397257805.\n",
            "Epoch 68/200\n",
            "1562/1562 [==============================] - 119s 76ms/step - loss: 0.2419 - accuracy: 0.9144 - val_loss: 0.3642 - val_accuracy: 0.8860 - lr: 0.0027\n",
            "\n",
            "Epoch 00069: LearningRateScheduler setting learning rate to 0.002428206382319331.\n",
            "Epoch 69/200\n",
            "1562/1562 [==============================] - 119s 76ms/step - loss: 0.2368 - accuracy: 0.9168 - val_loss: 0.3512 - val_accuracy: 0.8914 - lr: 0.0024\n",
            "\n",
            "Epoch 00070: LearningRateScheduler setting learning rate to 0.0023067960632033647.\n",
            "Epoch 70/200\n",
            "1562/1562 [==============================] - 119s 76ms/step - loss: 0.2288 - accuracy: 0.9191 - val_loss: 0.3500 - val_accuracy: 0.8934 - lr: 0.0023\n",
            "\n",
            "Epoch 00071: LearningRateScheduler setting learning rate to 0.00230679614469409.\n",
            "Epoch 71/200\n",
            "1562/1562 [==============================] - 119s 76ms/step - loss: 0.2295 - accuracy: 0.9196 - val_loss: 0.3263 - val_accuracy: 0.8987 - lr: 0.0023\n",
            "\n",
            "Epoch 00072: LearningRateScheduler setting learning rate to 0.00230679614469409.\n",
            "Epoch 72/200\n",
            "1562/1562 [==============================] - 119s 76ms/step - loss: 0.2277 - accuracy: 0.9197 - val_loss: 0.3548 - val_accuracy: 0.8917 - lr: 0.0023\n",
            "\n",
            "Epoch 00073: LearningRateScheduler setting learning rate to 0.00230679614469409.\n",
            "Epoch 73/200\n",
            "1562/1562 [==============================] - 121s 77ms/step - loss: 0.2297 - accuracy: 0.9204 - val_loss: 0.3680 - val_accuracy: 0.8845 - lr: 0.0023\n",
            "\n",
            "Epoch 00074: LearningRateScheduler setting learning rate to 0.00230679614469409.\n",
            "Epoch 74/200\n",
            "1562/1562 [==============================] - 120s 77ms/step - loss: 0.2270 - accuracy: 0.9195 - val_loss: 0.3636 - val_accuracy: 0.8895 - lr: 0.0023\n",
            "\n",
            "Epoch 00075: LearningRateScheduler setting learning rate to 0.0020761166233569384.\n",
            "Epoch 75/200\n",
            "1562/1562 [==============================] - 119s 76ms/step - loss: 0.2206 - accuracy: 0.9238 - val_loss: 0.3251 - val_accuracy: 0.8999 - lr: 0.0021\n",
            "\n",
            "Epoch 00076: LearningRateScheduler setting learning rate to 0.0020761166233569384.\n",
            "Epoch 76/200\n",
            "1562/1562 [==============================] - 118s 76ms/step - loss: 0.2240 - accuracy: 0.9208 - val_loss: 0.3285 - val_accuracy: 0.8969 - lr: 0.0021\n",
            "\n",
            "Epoch 00077: LearningRateScheduler setting learning rate to 0.0020761166233569384.\n",
            "Epoch 77/200\n",
            "1562/1562 [==============================] - 122s 78ms/step - loss: 0.2183 - accuracy: 0.9228 - val_loss: 0.3329 - val_accuracy: 0.8967 - lr: 0.0021\n",
            "\n",
            "Epoch 00078: LearningRateScheduler setting learning rate to 0.0020761166233569384.\n",
            "Epoch 78/200\n",
            "1562/1562 [==============================] - 119s 76ms/step - loss: 0.2145 - accuracy: 0.9241 - val_loss: 0.3460 - val_accuracy: 0.8958 - lr: 0.0021\n",
            "\n",
            "Epoch 00079: LearningRateScheduler setting learning rate to 0.0018685049144551158.\n",
            "Epoch 79/200\n",
            "1562/1562 [==============================] - 119s 76ms/step - loss: 0.2140 - accuracy: 0.9243 - val_loss: 0.3424 - val_accuracy: 0.8918 - lr: 0.0019\n",
            "\n",
            "Epoch 00080: LearningRateScheduler setting learning rate to 0.00177507966873236.\n",
            "Epoch 80/200\n",
            "1562/1562 [==============================] - 119s 76ms/step - loss: 0.2085 - accuracy: 0.9273 - val_loss: 0.3237 - val_accuracy: 0.9018 - lr: 0.0018\n"
          ]
        }
      ],
      "source": [
        "# model.fit(X_train, y_train,\n",
        "#                     batch_size=batch_size,\n",
        "#                     epochs=epochs,\n",
        "#                     verbose=1, \n",
        "#                     validation_data=(X_test, y_test))\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "steps_per_epoch = x_train.shape[0] // batch_size\n",
        "callbacks_list = [NanStop, lrschedule, reduce_lr, checkpoint, stop_training]\n",
        "model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        batch_size = batch_size,\n",
        "        epochs=200,\n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks= callbacks_list)\n",
        "model.save_weights('first_try.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "ZcWydmIVhZGr",
        "outputId": "a0345aa5-79ff-4e56-eb94-50437b43c4fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 66s 7ms/sample - loss: 1.4938 - accuracy: 0.4969\n",
            "Test loss: 1.493803402900696\n",
            "Test accuracy: 0.4969\n"
          ]
        }
      ],
      "source": [
        "# Test the model --------- original\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BMpMRgall-U",
        "outputId": "ad17f5f5-34fe-4949-f88a-7befe9a4a671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 68s 7ms/step - loss: 0.3237 - accuracy: 0.9018\n"
          ]
        }
      ],
      "source": [
        "# Test the model \n",
        "score = model.evaluate(x_test, y_test, verbose=1, use_multiprocessing=True, steps=10000, callbacks= callbacks_list)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE3lF6EH1r_L"
      },
      "outputs": [],
      "source": [
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"DNST_model.h5\")\n",
        "print(\"Saved model to disk\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DenseNet_cifar10_final_(1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}